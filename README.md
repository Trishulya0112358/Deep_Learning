# Deep_Learning
Basic Codes for a Deep Neural Network

These codes implement Neural Networks from scratch in Python.
The Forward and Backward propagation steps are all implemented as functions which take in appropriate input parameters and return output corresponding results.

# 1] Deep Neural Network Template 1:

   a] Initialized parameters

   b] Performed the matrix multiplication step

   c] Computed the activation value

   d] Performed the feed-forward step

   e] Computed the Cost function
 
   f] Performed back propagation
   
   g] Updated Parameters
   
   
# 2] L-layer Deep NN Template:

   a] Used previously implemented functions to build a 2-layer deep and an L-layer Deep Neural Network.

   b] Implemented Caching of parameters

   c] Use case is Image Classification
   
# 3] Planar Data Classification:

   a] Implemented Hyperparameter Tuning
   
   b] Used the previously built Neural Networks with different Hyperparameters and plot the results
   
# 4] Logistic Regression NN template:
   
   a] Implemented Gradient descent and optimize the model.
   
   b] Use Case: Image Classification
   
# 5] Gradient Checking:
   
   a] Implemented Gradient Checking
   
   b] Ensured that the formulae and values used in Gradient Descent are correct
  
# 6] L2 Regularization and Dropout Implementation:
   
   a] Neural networks face the problem of high variance, they overfit training data
   
   b] L2 regularization ensures the parameters in training are not heavily affected by some kind of pattern in training data
   
   c] Dropout regularization ensures the output does not depend on a few select nodes in the entire network
   
   d] Implemented L2 Regularization Cost function
   
   e] Implemented Forward and backward propagation with Dropout
  
# 7] Optimization Methods Momentum Mini-batch Adam:
   
   a] Implemented Random mini-batch sampling and forward and backward propagation steps.
   
   b] Implemented Adam (Adaptive Momentum Estimation) Optimizer
   
# 8] Basic RNN and LSTM:
   
   a] Implemented the individual RNN and LSTM cells
   
   b] Implemented Forward propagation for both architectures
